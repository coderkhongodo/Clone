#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import subprocess
import sys
import os

def run_training(model_type='base', use_wandb=False, epochs=5, batch_size=16):
    """
    Ch·∫°y training PhoBERT v·ªõi c·∫•u h√¨nh ƒë∆∞·ª£c ƒë·ªãnh s·∫µn
    
    Args:
        model_type: 'base' ho·∫∑c 'large'
        use_wandb: C√≥ s·ª≠ d·ª•ng wandb logging kh√¥ng
        epochs: S·ªë epochs
        batch_size: Batch size
    """
    
    # X√°c ƒë·ªãnh model name
    if model_type == 'base':
        model_name = 'vinai/phobert-base'
        output_dir = 'checkpoints/phobert-base-v2'
    elif model_type == 'large':
        model_name = 'vinai/phobert-large'
        output_dir = 'checkpoints/phobert-large-v2'
    else:
        print("ERROR: model_type ph·∫£i l√† 'base' ho·∫∑c 'large'")
        return
    
    # Model-specific optimized hyperparameters for maximum F1
    if model_type == 'base':
        learning_rate = '8e-6'
        dropout_rate = '0.35'
        freeze_layers = '6'
        grad_accumulation = '4'
        actual_batch_size = batch_size
    else:  # large
        learning_rate = '5e-6'  # Lower LR for large model
        dropout_rate = '0.45'   # Higher dropout for large model
        freeze_layers = '10'    # Freeze more layers
        grad_accumulation = '8' # Larger accumulation
        actual_batch_size = max(4, batch_size // 2)  # Smaller batch for memory
    
    # T·∫°o command v·ªõi advanced hyperparameters for higher F1
    cmd = [
        'python', 'train_phobert.py',
        '--model_name', model_name,
        '--output_dir', output_dir,
        '--epochs', str(epochs),
        '--batch_size', str(actual_batch_size),
        '--learning_rate', learning_rate,
        '--weight_decay', '0.01',
        '--warmup_ratio', '0.2',     # More warmup
        '--scheduler', 'cosine',     # Cosine scheduler
        '--loss_type', 'f1',         # Direct F1 optimization
        '--dropout_rate', dropout_rate,
        '--num_freeze_layers', freeze_layers,
        '--gradient_accumulation_steps', grad_accumulation,
        '--pooling_strategy', 'cls_mean',  # Better pooling
        '--use_amp',  # Mixed precision
        '--use_ema',  # Exponential moving average
        '--ema_decay', '0.9999',     # EMA decay
        '--patience', '8'  # More patience for better convergence
    ]
    
    if use_wandb:
        cmd.extend(['--use_wandb', '--wandb_project', 'phobert-clickbait'])
    
    print(f"START: Starting training PhoBERT-{model_type} with F1-optimized settings")
    print(f"STATS: Config: {epochs} epochs, batch={actual_batch_size}, lr={learning_rate}")
    print(f"TARGET: Advanced: dropout={dropout_rate}, freeze={freeze_layers}, grad_accum={grad_accumulation}")
    print(f"CONFIG: Features: EMA, F1-loss, cls_mean pooling, cosine scheduler")
    print(f"üìÅ Output: {output_dir}")
    print(f"METRICS: Wandb: {'Yes' if use_wandb else 'No'}")
    print()
    
    # Ch·∫°y training
    try:
        subprocess.run(cmd, check=True)
        print("SUCCESS: Training completed successfully!")
    except subprocess.CalledProcessError as e:
        print(f"ERROR: Training failed with error: {e}")
        return False
    except KeyboardInterrupt:
        print("‚èπÔ∏è Training interrupted by user")
        return False
    
    return True

def main():
    print("=== PHOBERT TRAINING LAUNCHER ===")
    print()
    
    # Ki·ªÉm tra d·ªØ li·ªáu ƒë√£ preprocessing ch∆∞a
    if not os.path.exists('data-bert-v2'):
        print("ERROR: Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ƒë√£ preprocessing!")
        print("TIP: Ch·∫°y: python preprocess_for_phobert.py")
        return
    
    print("Ch·ªçn lo·∫°i model:")
    print("1. PhoBERT-base (nh·∫π h∆°n, nhanh h∆°n)")
    print("2. PhoBERT-large (ch√≠nh x√°c h∆°n, ch·∫≠m h∆°n)")
    print("3. C·∫£ hai (train base tr∆∞·ªõc, sau ƒë√≥ large)")
    
    choice = input("\nNh·∫≠p l·ª±a ch·ªçn (1/2/3): ").strip()
    
    # C·∫•u h√¨nh training
    print("\n=== C·∫§U H√åNH TRAINING ===")
    
    try:
        epochs = int(input("S·ªë epochs (default: 5): ") or "5")
        batch_size = int(input("Batch size (default: 16): ") or "16")
    except ValueError:
        print("ERROR: Input kh√¥ng h·ª£p l·ªá, s·ª≠ d·ª•ng default values")
        epochs, batch_size = 5, 16
    
    use_wandb_input = input("S·ª≠ d·ª•ng wandb logging? (y/n, default: n): ").strip().lower()
    use_wandb = use_wandb_input in ['y', 'yes', '1']
    
    print(f"\nSTATS: C·∫•u h√¨nh:")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    print(f"   Wandb: {'Yes' if use_wandb else 'No'}")
    
    confirm = input("\nB·∫Øt ƒë·∫ßu training? (y/n): ").strip().lower()
    if confirm not in ['y', 'yes', '1']:
        print("ERROR: Training cancelled")
        return
    
    # B·∫Øt ƒë·∫ßu training
    if choice == '1':
        run_training('base', use_wandb, epochs, batch_size)
    elif choice == '2':
        run_training('large', use_wandb, epochs, batch_size)
    elif choice == '3':
        print("PROCESS: Training PhoBERT-base first...")
        success = run_training('base', use_wandb, epochs, batch_size)
        if success:
            print("\nPROCESS: Training PhoBERT-large...")
            run_training('large', use_wandb, epochs, batch_size)
    else:
        print("ERROR: L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá")

if __name__ == "__main__":
    main() 