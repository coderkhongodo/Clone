"""
Data Augmentation for Imbalanced Dataset using Synonym Replacement
S·ª≠ d·ª•ng Synonym Replacement ƒë·ªÉ tƒÉng c∆∞·ªùng d·ªØ li·ªáu cho class minority (clickbait)
Kh√¥ng c·∫ßn external libraries ph·ª©c t·∫°p
"""

import os
import pandas as pd
import numpy as np
import json
import re
import random
from tqdm import tqdm
from typing import List, Dict, Tuple
import warnings
warnings.filterwarnings("ignore")

class VietnameseSynonymReplacer:
    """
    Vietnamese Synonym Replacement using comprehensive predefined synonyms
    """
    
    def __init__(self):
        print("LOG: Initializing Vietnamese Synonym Replacer...")
        
        # Extended Vietnamese synonyms dictionary
        self.synonyms = {
            # Clickbait-specific words (quan tr·ªçng nh·∫•t)
            "b√≠ m·∫≠t": ["ƒëi·ªÅu b√≠ ·∫©n", "b√≠ ·∫©n", "ƒëi·ªÅu k√≠n", "ƒëi·ªÅu ·∫©n gi·∫•u", "ƒëi·ªÅu kh√¥ng ai bi·∫øt", "ƒëi·ªÅu th·∫ßm k√≠n"],
            "g√¢y s·ªëc": ["g√¢y shock", "l√†m cho√°ng", "l√†m b·∫•t ng·ªù", "khi·∫øn s·ª≠ng s·ªët", "g√¢y ng·ª° ng√†ng", "l√†m kinh ng·∫°c"],
            "kh√¥ng ai ng·ªù": ["b·∫•t ng·ªù", "kh√¥ng ai bi·∫øt", "ngo√†i d·ª± ƒëo√°n", "kh√¥ng l∆∞·ªùng tr∆∞·ªõc", "b·∫•t th·∫ßn", "ƒë·ªôt ng·ªôt"],
            "kh√≥ tin": ["kh√¥ng th·ªÉ tin", "phi th∆∞·ªùng", "b·∫•t th∆∞·ªùng", "l·∫° l√πng", "k·ª≥ l·∫°", "kh√¥ng tin n·ªïi"],
            "kinh ho√†ng": ["kh·ªßng khi·∫øp", "ƒë√°ng s·ª£", "gh√™ r·ª£n", "r√πng r·ª£n", "kinh d·ªã", "khi·∫øp s·ª£"],
            "b·∫•t ng·ªù": ["ƒë·ªôt ng·ªôt", "kh√¥ng ng·ªù", "ngo√†i d·ª± t√≠nh", "ngo√†i √Ω mu·ªën", "kh√¥ng l∆∞·ªùng tr∆∞·ªõc"],
            "s·ª± th·∫≠t": ["th·ª±c t·∫ø", "th·∫≠t", "ƒëi·ªÅu th·∫≠t", "th·ª±c ch·∫•t", "hi·ªán th·ª±c", "s·ª± th·ª±c"],
            "cho√°ng v√°ng": ["s·ªØng s·ªù", "ng·ª° ng√†ng", "b·∫•t ng·ªù", "s·ªëc", "kinh ng·∫°c", "t√° h·ªèa"],
            
            # Numbers and rankings (common in clickbait)
            "top": ["h√†ng ƒë·∫ßu", "t·ªët nh·∫•t", "ƒë·ªânh", "cao nh·∫•t", "s·ªë 1", "ƒë·ª©ng ƒë·∫ßu"],
            "t·ªët nh·∫•t": ["hay nh·∫•t", "xu·∫•t s·∫Øc nh·∫•t", "ƒë·ªânh cao", "s·ªë m·ªôt", "h√†ng ƒë·∫ßu"],
            "x·∫•u nh·∫•t": ["t·ªá nh·∫•t", "d·ªü nh·∫•t", "k√©m nh·∫•t", "t·ªìi t·ªá nh·∫•t", "kinh kh·ªßng nh·∫•t"],
            
            # Common adjectives
            "t·ªët": ["hay", "gi·ªèi", "xu·∫•t s·∫Øc", "∆∞u vi·ªát", "tuy·ªát v·ªùi", "ho√†n h·∫£o"],
            "x·∫•u": ["t·ªá", "d·ªü", "k√©m", "t·ªìi t·ªá", "kinh kh·ªßng", "th·∫£m h·∫°i"],
            "l·ªõn": ["to", "kh·ªïng l·ªì", "r·ªông l·ªõn", "ƒë·ªì s·ªô", "vƒ© ƒë·∫°i", "kh·ªßng"],
            "nh·ªè": ["b√©", "t√≠ hon", "thu nh·ªè", "t√≠ x√≠u", "li ti", "nh√≠t"],
            "ƒë·∫πp": ["xinh", "lung linh", "quy·∫øn r≈©", "h·∫•p d·∫´n", "tuy·ªát ƒë·∫πp", "r·ª±c r·ª°"],
            "nhi·ªÅu": ["ƒë√¥ng", "m·ªçi", "r·∫•t nhi·ªÅu", "v√¥ s·ªë", "h√†ng lo·∫°t", "v√¥ v√†n"],
            "√≠t": ["thi·∫øu", "hi·∫øm", "khan hi·∫øm", "kh√¥ng nhi·ªÅu", "s·ªë √≠t", "eo h·∫πp"],
            
            # News-related words
            "tin t·ª©c": ["th√¥ng tin", "b√°o tin", "tin b√°o", "th√¥ng b√°o", "b·∫£n tin", "tin b√†i"],
            "s·ª± ki·ªán": ["vi·ªác", "chuy·ªán", "ƒëi·ªÅu", "c√¢u chuy·ªán", "t√¨nh hu·ªëng", "bi·∫øn c·ªë"],
            "ng∆∞·ªùi": ["c√° nh√¢n", "con ng∆∞·ªùi", "nh√¢n v·∫≠t", "ai ƒë√≥", "ƒë·ªëi t∆∞·ª£ng", "c∆∞ d√¢n"],
            "n·ªïi ti·∫øng": ["n·ªïi b·∫≠t", "ƒë∆∞·ª£c bi·∫øt ƒë·∫øn", "c√≥ ti·∫øng", "danh ti·∫øng", "n·ªïi danh", "n·ª©c ti·∫øng"],
            "th√†nh c√¥ng": ["chi·∫øn th·∫Øng", "ƒë·∫°t ƒë∆∞·ª£c", "ho√†n th√†nh", "gi√†nh ƒë∆∞·ª£c", "ƒë·∫°t th√†nh t√≠ch", "th·∫Øng l·ª£i"],
            
            # Time-related words
            "m·ªõi": ["m·ªõi m·∫ª", "t∆∞∆°i m·ªõi", "m·ªõi l·∫°", "hi·ªán ƒë·∫°i", "c·∫≠p nh·∫≠t", "v·ª´a m·ªõi"],
            "c≈©": ["l·ªói th·ªùi", "x∆∞a", "c·ªï", "qu√° kh·ª©", "ƒë√£ qua", "l·∫°c h·∫≠u"],
            "nhanh": ["ch·ªõp nho√°ng", "th·∫ßn t·ªëc", "t·ª©c th√¨", "nhanh ch√≥ng", "mau l·∫π", "t·ªëc ƒë·ªô"],
            "ch·∫≠m": ["t·ª´ t·ª´", "ch·∫≠m ch·∫°p", "√¨ ·∫°ch", "ch·∫≠m r√£i", "l·ªÅ m·ªÅ", "·∫≠m ·∫°ch"],
            
            # Emotional words
            "vui": ["vui v·∫ª", "h·∫°nh ph√∫c", "ph·∫•n kh√≠ch", "h√†o h·ª©ng", "th√≠ch th√∫", "vui m·ª´ng"],
            "bu·ªìn": ["u s·∫ßu", "th·∫•t v·ªçng", "ƒëau kh·ªï", "ch√°n n·∫£n", "·ªß r≈©", "s·∫ßu mu·ªôn"],
            "gi·∫≠n": ["t·ª©c gi·∫≠n", "b·ª±c t·ª©c", "ph·∫´n n·ªô", "kh√≥ ch·ªãu", "c√°u k·ªânh", "c√°u g·∫Øt"],
            "s·ª£": ["lo s·ª£", "ho·∫£ng s·ª£", "kinh s·ª£", "e ng·∫°i", "lo l·∫Øng", "s·ª£ h√£i"],
            
            # Action words
            "l√†m": ["th·ª±c hi·ªán", "ti·∫øn h√†nh", "t·∫°o ra", "th·ª±c thi", "c·∫£i thi·ªán", "ph√°t tri·ªÉn"],
            "c√≥": ["s·ªü h·ªØu", "t·ªìn t·∫°i", "hi·ªán c√≥", "ch·ª©a ƒë·ª±ng", "bao g·ªìm", "mang"],
            "ƒëi": ["di chuy·ªÉn", "ti·∫øn h√†nh", "r·ªùi kh·ªèi", "b∆∞·ªõc ƒëi", "chuy·ªÉn ƒë·ªông", "h√†nh tr√¨nh"],
            "ƒë·∫øn": ["t·ªõi", "ƒë·∫°t t·ªõi", "ti·∫øp c·∫≠n", "ti·∫øn ƒë·∫øn", "t·ªõi n∆°i", "v·ªÅ"],
            
            # Vietnamese-specific words
            "vi·ªát nam": ["vi·ªát", "vn", "trong n∆∞·ªõc", "ƒë·∫•t n∆∞·ªõc", "qu√™ h∆∞∆°ng", "t·ªï qu·ªëc"],
            "th·∫ø gi·ªõi": ["to√†n c·∫ßu", "qu·ªëc t·∫ø", "ƒë·ªãa c·∫ßu", "tr√°i ƒë·∫•t", "nh√¢n lo·∫°i", "to√†n th·∫ø gi·ªõi"],
            "h√† n·ªôi": ["th·ªß ƒë√¥", "h√† th√†nh", "thƒÉng long", "khu v·ª±c h√† n·ªôi"],
            "tp hcm": ["s√†i g√≤n", "th√†nh ph·ªë h·ªì ch√≠ minh", "hcm", "th∆∞∆°ng c·∫£ng"],
            
            # Technology words
            "c√¥ng ngh·ªá": ["k·ªπ thu·∫≠t", "khoa h·ªçc", "ti·∫øn b·ªô", "ƒë·ªïi m·ªõi", "innovation", "tech"],
            "internet": ["m·∫°ng", "online", "web", "tr·ª±c tuy·∫øn", "m·∫°ng l∆∞·ªõi", "digital"],
            "ƒëi·ªán tho·∫°i": ["di ƒë·ªông", "smartphone", "mobile", "thi·∫øt b·ªã", "m√°y"],
            
            # Business words
            "c√¥ng ty": ["doanh nghi·ªáp", "t·∫≠p ƒëo√†n", "t·ªï ch·ª©c", "firm", "c√¥ng ty", "business"],
            "ti·ªÅn": ["ƒë·ªìng ti·ªÅn", "t√†i ch√≠nh", "thu nh·∫≠p", "l∆∞∆°ng", "kinh t·∫ø", "ng√¢n s√°ch"],
            "gi√°": ["m·ª©c gi√°", "chi ph√≠", "cost", "price", "gi√° c·∫£", "ph√≠"],
            
            # Sports words
            "b√≥ng ƒë√°": ["football", "soccer", "th·ªÉ thao", "m√¥n th·ªÉ thao", "t√∫c c·∫ßu"],
            "v√¥ ƒë·ªãch": ["chi·∫øn th·∫Øng", "ƒëƒÉng quang", "th·∫Øng l·ªõn", "gi√†nh cup", "champion"],
        }
        
        # Create reverse mapping for better coverage
        self.reverse_synonyms = {}
        for word, synonyms in self.synonyms.items():
            for synonym in synonyms:
                if synonym not in self.reverse_synonyms:
                    self.reverse_synonyms[synonym] = []
                self.reverse_synonyms[synonym].append(word)
        
        print(f"SUCCESS: Vietnamese Synonym Replacer initialized with {len(self.synonyms)} main words!")
    
    def get_synonyms(self, word: str) -> List[str]:
        """Get synonyms for a word"""
        word_lower = word.lower().strip()
        
        # Check main dictionary
        if word_lower in self.synonyms:
            return self.synonyms[word_lower]
        
        # Check reverse dictionary
        if word_lower in self.reverse_synonyms:
            return self.reverse_synonyms[word_lower]
        
        return []
    
    def synonym_replacement(self, text: str, replacement_prob: float = 0.4) -> str:
        """
        Replace words with their synonyms with higher probability for clickbait keywords
        """
        words = text.split()
        augmented_words = []
        
        # Clickbait keywords get higher replacement probability
        clickbait_keywords = ["b√≠ m·∫≠t", "g√¢y s·ªëc", "kh√¥ng ai ng·ªù", "kh√≥ tin", "kinh ho√†ng", 
                             "b·∫•t ng·ªù", "s·ª± th·∫≠t", "cho√°ng v√°ng", "top", "t·ªët nh·∫•t", "x·∫•u nh·∫•t"]
        
        for word in words:
            # Clean word (remove punctuation for lookup)
            clean_word = re.sub(r'[^\w\s]', '', word.lower())
            synonyms = self.get_synonyms(clean_word)
            
            # Higher probability for clickbait keywords
            prob = 0.7 if clean_word in clickbait_keywords else replacement_prob
            
            if synonyms and random.random() < prob:
                # Replace with random synonym
                synonym = random.choice(synonyms)
                
                # Preserve capitalization
                if len(word) > 0 and word[0].isupper():
                    synonym = synonym.capitalize()
                
                # Preserve punctuation
                punctuation = re.findall(r'[^\w\s]', word)
                if punctuation:
                    synonym += ''.join(punctuation)
                
                augmented_words.append(synonym)
            else:
                augmented_words.append(word)
        
        return ' '.join(augmented_words)

class DataAugmenter:
    """
    Data Augmenter using only Synonym Replacement
    """
    
    def __init__(self):
        print("START: Initializing Data Augmenter (Synonym Replacement Only)...")
        self.synonym_replacer = VietnameseSynonymReplacer()
        print("SUCCESS: Data Augmenter ready!")
    
    def augment_text(self, text: str, num_variants: int = 1) -> List[str]:
        """
        Generate multiple variants of the same text
        """
        variants = []
        
        for _ in range(num_variants):
            # Apply synonym replacement with some randomness
            variant = self.synonym_replacer.synonym_replacement(text, replacement_prob=random.uniform(0.3, 0.6))
            
            # Make sure it's different from original
            if variant != text and len(variant.strip()) > 0:
                variants.append(variant)
        
        return variants
    
    def create_alpaca_format(self, title: str, label: str) -> Dict:
        """
        Create Alpaca format for a sample
        """
        instruction = """B·∫°n l√† chuy√™n gia ph√¢n t√≠ch b√°o ch√≠ Vi·ªát Nam. H√£y ph√¢n lo·∫°i ti√™u ƒë·ªÅ b√†i b√°o th√†nh clickbait ho·∫∑c non-clickbait.

Ti√™u ch√≠ ph√¢n lo·∫°i:
- CLICKBAIT: Ti√™u ƒë·ªÅ c√¢u view, s·ª≠ d·ª•ng t·ª´ ng·ªØ c·∫£m x√∫c m·∫°nh, t·∫°o t√≤ m√≤ qu√° m·ª©c, thi·∫øu th√¥ng tin c·ª• th·ªÉ, c√≥ t·ª´ kh√≥a nh∆∞ "b√≠ m·∫≠t", "g√¢y s·ªëc", "kh√¥ng ai ng·ªù", "top X"
- NON-CLICKBAIT: Ti√™u ƒë·ªÅ th√¥ng tin r√µ r√†ng, kh√°ch quan, trung th·ª±c, c√≥ n·ªôi dung c·ª• th·ªÉ, mang t√≠nh tin t·ª©c th·ª±c t·∫ø

H√£y ph√¢n t√≠ch v√† ƒë∆∞a ra k·∫øt lu·∫≠n ch√≠nh x√°c."""

        input_text = f"Ph√¢n lo·∫°i ti√™u ƒë·ªÅ sau: {title}"
        
        if label == "clickbait":
            output_text = f"ƒê√¢y l√† ti√™u ƒë·ªÅ clickbait v√¨ s·ª≠ d·ª•ng ng√¥n ng·ªØ c√¢u view v√† t·∫°o t√≤ m√≤ qu√° m·ª©c.\n\nK·∫øt qu·∫£: clickbait"
        else:
            output_text = f"ƒê√¢y l√† ti√™u ƒë·ªÅ non-clickbait v√¨ cung c·∫•p th√¥ng tin r√µ r√†ng v√† kh√°ch quan.\n\nK·∫øt qu·∫£: non-clickbait"
        
        return {
            "instruction": instruction,
            "input": input_text,
            "output": output_text
        }
    
    def convert_csv_to_alpaca_json(self, csv_file: str, json_file: str):
        """
        Convert CSV file to Alpaca JSON format
        """
        df = pd.read_csv(csv_file)
        alpaca_data = []
        
        for _, row in df.iterrows():
            alpaca_sample = self.create_alpaca_format(row['title'], row['label'])
            alpaca_data.append(alpaca_sample)
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(alpaca_data, f, ensure_ascii=False, indent=2)
        
        print(f"REPORT: Alpaca JSON saved to: {json_file}")
        return len(alpaca_data)
    
    def augment_dataset(self, 
                       input_dir: str, 
                       output_dir: str, 
                       target_class: str = "clickbait",
                       augmentation_ratio: float = 2.0):
        """
        Augment dataset for minority class
        """
        print(f"\nPROCESS: Starting dataset augmentation...")
        print(f"üìÅ Input: {input_dir}")
        print(f"üìÅ Output: {output_dir}")
        print(f"TARGET: Target class: {target_class}")
        print(f"METRICS: Augmentation ratio: {augmentation_ratio}x")
        print(f"üõ†Ô∏è Method: Synonym Replacement Only")
        
        # Create output directory
        os.makedirs(output_dir, exist_ok=True)
        
        # Process only train split (val and test should remain original)
        for split in ["train"]:
            print(f"\nSTATS: Processing {split} split...")
            
            input_file = os.path.join(input_dir, split, f"{split}.csv")
            if not os.path.exists(input_file):
                print(f"WARNING: File not found: {input_file}")
                continue
            
            # Load data
            df = pd.read_csv(input_file)
            print(f"SUCCESS: Loaded {len(df)} samples")
            
            # Show class distribution
            class_counts = df['label'].value_counts()
            print(f"STATS: Class distribution:")
            for label, count in class_counts.items():
                print(f"   {label}: {count} ({count/len(df)*100:.1f}%)")
            
            # Generate augmented data for minority class
            if target_class in class_counts.index:
                minority_data = df[df['label'] == target_class].copy()
                n_minority = len(minority_data)
                n_to_generate = int(n_minority * (augmentation_ratio - 1))
                
                print(f"TARGET: Generating {n_to_generate} augmented samples for {target_class}")
                
                augmented_samples = []
                generated_count = 0
                
                # Generate multiple variants per sample
                samples_per_original = max(1, n_to_generate // n_minority)
                
                for idx, (_, sample) in tqdm(enumerate(minority_data.iterrows()), 
                                          total=len(minority_data), 
                                          desc=f"Augmenting {split}"):
                    original_title = sample['title']
                    
                    # Generate variants
                    variants = self.augment_text(original_title, num_variants=samples_per_original + 1)
                    
                    for variant in variants:
                        if generated_count >= n_to_generate:
                            break
                            
                        if variant != original_title and len(variant.strip()) > 0:
                            augmented_samples.append({
                                'title': variant,
                                'label': target_class,
                                'original_title': original_title,
                                'augmentation_method': 'synonym_replacement'
                            })
                            generated_count += 1
                    
                    if generated_count >= n_to_generate:
                        break
                
                print(f"SUCCESS: Generated {len(augmented_samples)} valid augmented samples")
                
                # Show some examples
                if len(augmented_samples) > 0:
                    print(f"\nüìñ Examples of augmented data:")
                    for i in range(min(3, len(augmented_samples))):
                        sample = augmented_samples[i]
                        print(f"   Original: {sample['original_title']}")
                        print(f"   Augmented: {sample['title']}")
                        print()
                
                # Combine original and augmented data
                augmented_df = pd.DataFrame(augmented_samples)
                combined_df = pd.concat([df, augmented_df[['title', 'label']]], ignore_index=True)
                
                # Shuffle
                combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)
                
                print(f"STATS: Final dataset size: {len(combined_df)} samples")
                
                # Show new class distribution
                new_class_counts = combined_df['label'].value_counts()
                print(f"STATS: New class distribution:")
                for label, count in new_class_counts.items():
                    percentage = count / len(combined_df) * 100
                    print(f"   {label}: {count} ({percentage:.1f}%)")
                
                # Save combined dataset
                output_split_dir = os.path.join(output_dir, split)
                os.makedirs(output_split_dir, exist_ok=True)
                
                # Save CSV format
                output_csv_file = os.path.join(output_split_dir, f"{split}.csv")
                combined_df.to_csv(output_csv_file, index=False)
                print(f"SAVE: CSV saved to: {output_csv_file}")
                
                # Save Alpaca JSON format
                output_json_file = os.path.join(output_split_dir, f"{split}_alpaca.json")
                json_samples = self.convert_csv_to_alpaca_json(output_csv_file, output_json_file)
                
                # Save augmentation details
                if len(augmented_samples) > 0:
                    details_file = os.path.join(output_split_dir, f"{split}_augmentation_details.csv")
                    augmented_df.to_csv(details_file, index=False)
                    print(f"REPORT: Augmentation details saved to: {details_file}")
            
            else:
                print(f"WARNING: Target class '{target_class}' not found in {split}")
                # Just copy original files
                output_split_dir = os.path.join(output_dir, split)
                os.makedirs(output_split_dir, exist_ok=True)
                
                # Save CSV format
                output_csv_file = os.path.join(output_split_dir, f"{split}.csv")
                df.to_csv(output_csv_file, index=False)
                print(f"REPORT: CSV copied to: {output_csv_file}")
                
                # Save Alpaca JSON format
                output_json_file = os.path.join(output_split_dir, f"{split}_alpaca.json")
                json_samples = self.convert_csv_to_alpaca_json(output_csv_file, output_json_file)

def main():
    """
    Main augmentation process
    """
    print("=" * 80)
    print("PROCESS: DATA AUGMENTATION: VIETNAMESE SYNONYM REPLACEMENT")
    print("REPORT: Creates both CSV and Alpaca JSON formats")
    print("WARNING:  ONLY augments TRAIN set - Val/Test remain original")
    print("=" * 80)
    
    # Configuration
    input_dir = "simple_dataset"
    output_dir = "data_genUpsampling"
    target_class = "clickbait"  # Minority class to augment
    augmentation_ratio = 2.2    # 2.2x tƒÉng class clickbait
    
    print(f"üìÅ Input directory: {input_dir}")
    print(f"üìÅ Output directory: {output_dir}")
    print(f"TARGET: Target class: {target_class}")
    print(f"METRICS: Augmentation ratio: {augmentation_ratio}x")
    print(f"üõ†Ô∏è Method: Vietnamese Synonym Replacement")
    print(f"REPORT: Output formats: CSV + Alpaca JSON")
    print(f"WARNING:  Processing: TRAIN only (Val/Test keep original)")
    
    # Validate input directory
    if not os.path.exists(input_dir):
        print(f"ERROR: Input directory not found: {input_dir}")
        return
    
    try:
        # Initialize augmenter
        augmenter = DataAugmenter()
        
        # Run augmentation
        augmenter.augment_dataset(
            input_dir=input_dir,
            output_dir=output_dir,
            target_class=target_class,
            augmentation_ratio=augmentation_ratio
        )
        
        print(f"\nCOMPLETE: DATA AUGMENTATION COMPLETED!")
        print(f"üìÅ Augmented data saved to: {output_dir}")
        print(f"REPORT: Generated both CSV and Alpaca JSON formats:")
        print(f"   - *.csv files for traditional ML models")
        print(f"   - *_alpaca.json files for LLM fine-tuning")
        print(f"SEARCH: Check augmentation details in *_augmentation_details.csv files")
        print(f"TIP: To use with models:")
        print(f"   - Train: use data_genUpsampling/train/ (augmented)")
        print(f"   - Val: use original simple_dataset/val/val.csv (NO augmentation)")
        print(f"   - Test: use original simple_dataset/test/test.csv (NO augmentation)")
        print(f"   - Only TRAIN set is augmented, Val/Test remain original for proper evaluation")
        
    except Exception as e:
        print(f"ERROR: Error during augmentation: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 