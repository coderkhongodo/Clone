#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import pickle
import csv
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from tqdm import tqdm

# Import PhoBERT v√† VnCoreNLP
try:
    from transformers import AutoTokenizer
    from vncorenlp import VnCoreNLP
    from sklearn.preprocessing import LabelEncoder
    import torch
except ImportError as e:
    print(f"ERROR: L·ªói import: {e}")
    print("Vui l√≤ng c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt:")
    print("pip install -r requirements.txt")
    exit(1)

class PhoBERTPreprocessor:
    def __init__(self, model_name="vinai/phobert-base", max_length=256, vncorenlp_path=None):
        """
        Kh·ªüi t·∫°o preprocessor cho PhoBERT
        
        Args:
            model_name: "vinai/phobert-base" ho·∫∑c "vinai/phobert-large"
            max_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence
            vncorenlp_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn VnCoreNLP jar file
        """
        self.model_name = model_name
        self.max_length = max_length
        
        print(f"PROCESS: ƒêang kh·ªüi t·∫°o {model_name}...")
        
        # Kh·ªüi t·∫°o tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        
        # Kh·ªüi t·∫°o VnCoreNLP cho word segmentation
        self.setup_vncorenlp(vncorenlp_path)
        
        # Label encoder
        self.label_encoder = LabelEncoder()
        
        print("SUCCESS: Kh·ªüi t·∫°o ho√†n th√†nh!")
    
    def setup_vncorenlp(self, vncorenlp_path):
        """Setup VnCoreNLP cho word segmentation"""
        try:
            if vncorenlp_path and os.path.exists(vncorenlp_path):
                self.segmenter = VnCoreNLP(vncorenlp_path, annotators="wseg", max_heap_size='-Xmx500m')
            else:
                # T·ª± ƒë·ªông download VnCoreNLP
                print("PROCESS: ƒêang t·∫£i VnCoreNLP...")
                self.segmenter = VnCoreNLP(annotators="wseg", max_heap_size='-Xmx500m')
            print("SUCCESS: VnCoreNLP ƒë√£ s·∫µn s√†ng!")
        except Exception as e:
            print(f"ERROR: L·ªói kh·ªüi t·∫°o VnCoreNLP: {e}")
            print("TIP: C√≥ th·ªÉ c·∫ßn download VnCoreNLP model:")
            print("vncorenlp download")
            self.segmenter = None
    
    def word_segment(self, text: str) -> str:
        """
        Word segmentation s·ª≠ d·ª•ng VnCoreNLP
        """
        if not self.segmenter:
            return text
        
        try:
            # VnCoreNLP word segmentation
            segmented = self.segmenter.tokenize(text)
            # N·ªëi c√°c t·ª´ ƒë√£ ph√¢n ƒëo·∫°n
            result = []
            for sentence in segmented:
                result.extend(sentence)
            return " ".join(result)
        except Exception as e:
            print(f"WARNING: L·ªói word segmentation: {e}")
            return text
    
    def preprocess_text(self, text: str) -> str:
        """
        Ti·ªÅn x·ª≠ l√Ω text cho PhoBERT
        """
        # L√†m s·∫°ch text c∆° b·∫£n
        text = text.strip()
        if not text:
            return ""
        
        # Word segmentation
        segmented_text = self.word_segment(text)
        
        return segmented_text
    
    def encode_texts(self, texts: List[str]) -> Dict:
        """
        Encode danh s√°ch texts th√†nh format cho PhoBERT
        """
        print(f"PROCESS: ƒêang encode {len(texts)} texts...")
        
        # Ti·ªÅn x·ª≠ l√Ω texts
        processed_texts = []
        for text in tqdm(texts, desc="Word segmentation"):
            processed_text = self.preprocess_text(text)
            processed_texts.append(processed_text)
        
        # Tokenize v·ªõi PhoBERT tokenizer
        print("PROCESS: ƒêang tokenize v·ªõi PhoBERT...")
        encoded = self.tokenizer(
            processed_texts,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'processed_texts': processed_texts
        }
    
    def encode_labels(self, labels: List[str]) -> Dict:
        """
        Encode labels
        """
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        return {
            'labels': torch.tensor(encoded_labels, dtype=torch.long),
            'label_mapping': {label: idx for idx, label in enumerate(self.label_encoder.classes_)},
            'num_classes': len(self.label_encoder.classes_)
        }
    
    def load_data(self, file_path: str) -> Tuple[List[str], List[str]]:
        """
        Load d·ªØ li·ªáu t·ª´ CSV ho·∫∑c JSONL
        """
        texts = []
        labels = []
        
        if file_path.endswith('.csv'):
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    texts.append(row['title'])
                    labels.append(row['label'])
        elif file_path.endswith('.jsonl'):
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    data = json.loads(line.strip())
                    texts.append(data['title'])
                    labels.append(data['label'])
        
        return texts, labels
    
    def save_processed_data(self, data: Dict, output_path: str):
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        """
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"SUCCESS: ƒê√£ l∆∞u: {output_path}")
    
    def process_dataset(self, input_dir: str, output_dir: str):
        """
        X·ª≠ l√Ω to√†n b·ªô dataset
        """
        print(f"=== TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU CHO {self.model_name.upper()} ===")
        print(f"Max length: {self.max_length}")
        print()
        
        # T·∫°o th∆∞ m·ª•c output
        os.makedirs(output_dir, exist_ok=True)
        
        splits = ['train', 'val', 'test']
        all_labels = []
        
        # Thu th·∫≠p t·∫•t c·∫£ labels ƒë·ªÉ fit label encoder
        print("PROCESS: ƒêang thu th·∫≠p labels...")
        for split in splits:
            csv_path = os.path.join(input_dir, split, f'{split}.csv')
            if os.path.exists(csv_path):
                _, labels = self.load_data(csv_path)
                all_labels.extend(labels)
        
        # Fit label encoder
        self.label_encoder.fit(all_labels)
        label_info = {
            'label_mapping': {label: idx for idx, label in enumerate(self.label_encoder.classes_)},
            'num_classes': len(self.label_encoder.classes_)
        }
        
        print(f"STATS: Label mapping: {label_info['label_mapping']}")
        print()
        
        # X·ª≠ l√Ω t·ª´ng split
        for split in splits:
            print(f"PROCESS: ƒêang x·ª≠ l√Ω {split} set...")
            
            csv_path = os.path.join(input_dir, split, f'{split}.csv')
            if not os.path.exists(csv_path):
                print(f"WARNING: Kh√¥ng t√¨m th·∫•y {csv_path}")
                continue
            
            # Load d·ªØ li·ªáu
            texts, labels = self.load_data(csv_path)
            print(f"STATS: {split}: {len(texts)} m·∫´u")
            
            # Encode texts
            encoded_texts = self.encode_texts(texts)
            
            # Encode labels
            encoded_labels = self.label_encoder.transform(labels)
            
            # T·∫°o dataset
            dataset = {
                'input_ids': encoded_texts['input_ids'],
                'attention_mask': encoded_texts['attention_mask'],
                'labels': torch.tensor(encoded_labels, dtype=torch.long),
                'original_texts': texts,
                'processed_texts': encoded_texts['processed_texts'],
                'original_labels': labels,
                'label_mapping': label_info['label_mapping'],
                'num_classes': label_info['num_classes']
            }
            
            # L∆∞u d·ªØ li·ªáu
            output_path = os.path.join(output_dir, f'{split}_processed.pkl')
            self.save_processed_data(dataset, output_path)
            
            # L∆∞u th√¥ng tin th·ªëng k√™
            stats = {
                'split': split,
                'num_samples': len(texts),
                'max_length': self.max_length,
                'model_name': self.model_name,
                'avg_length': np.mean([len(text.split()) for text in encoded_texts['processed_texts']]),
                'label_distribution': {label: labels.count(label) for label in set(labels)}
            }
            
            stats_path = os.path.join(output_dir, f'{split}_stats.json')
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            print(f"SUCCESS: Ho√†n th√†nh {split} set")
            print()
        
        # L∆∞u th√¥ng tin chung
        info = {
            'model_name': self.model_name,
            'max_length': self.max_length,
            'tokenizer_vocab_size': self.tokenizer.vocab_size,
            'label_mapping': label_info['label_mapping'],
            'num_classes': label_info['num_classes'],
            'preprocessing_info': {
                'word_segmentation': 'VnCoreNLP',
                'special_tokens': self.tokenizer.special_tokens_map
            }
        }
        
        info_path = os.path.join(output_dir, 'preprocessing_info.json')
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        
        print("SUCCESS: Ho√†n th√†nh ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu!")
        print(f"üìÅ D·ªØ li·ªáu ƒë√£ l∆∞u t·∫°i: {output_dir}")

def main():
    # C·∫•u h√¨nh - S·ª¨A ƒê·ªîI ƒê∆Ø·ªúNG D·∫™N
    INPUT_DIR = "simple_dataset"  # Th∆∞ m·ª•c ch·ª©a d·ªØ li·ªáu g·ªëc M·ªöI
    
    # X·ª≠ l√Ω cho PhoBERT-base-v2
    print("=== PHOBERT-BASE-V2 ===")
    processor_base = PhoBERTPreprocessor(
        model_name="vinai/phobert-base",
        max_length=256
    )
    processor_base.process_dataset(INPUT_DIR, "data-bert-v2/phobert-base-v2")
    
    print("\n" + "="*50 + "\n")
    
    # X·ª≠ l√Ω cho PhoBERT-large-v2
    print("=== PHOBERT-LARGE-V2 ===")
    processor_large = PhoBERTPreprocessor(
        model_name="vinai/phobert-large", 
        max_length=256
    )
    processor_large.process_dataset(INPUT_DIR, "data-bert-v2/phobert-large-v2")

if __name__ == "__main__":
    main() 