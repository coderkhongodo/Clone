#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import pickle
import csv
import subprocess
import tempfile
import uuid
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from tqdm import tqdm

# Import PhoBERT (khÃ´ng cáº§n VnCoreNLP thÆ° viá»‡n ná»¯a)
try:
    from transformers import AutoTokenizer
    from sklearn.preprocessing import LabelEncoder
    import torch
except ImportError as e:
    print(f"ERROR: Lá»—i import: {e}")
    print("Vui lÃ²ng cÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:")
    print("pip install -r requirements.txt")
    exit(1)

class LocalVnCoreNLP:
    """
    Local VnCoreNLP wrapper sá»­ dá»¥ng jar file thay vÃ¬ thÆ° viá»‡n Python
    """
    
    def __init__(self, jar_path: str, max_heap_size: str = '-Xmx1g'):
        """
        Khá»Ÿi táº¡o VnCoreNLP tá»« jar file local
        
        Args:
            jar_path: ÄÆ°á»ng dáº«n Ä‘áº¿n VnCoreNLP-1.1.1.jar
            max_heap_size: Heap size cho JVM (default: -Xmx1g)
        """
        self.jar_path = jar_path
        self.max_heap_size = max_heap_size
        
        if not os.path.exists(jar_path):
            raise FileNotFoundError(f"VnCoreNLP jar file khÃ´ng tÃ¬m tháº¥y: {jar_path}")
        
        # Test VnCoreNLP cÃ³ hoáº¡t Ä‘á»™ng khÃ´ng
        self._test_vncorenlp()
        
        print(f"âœ… VnCoreNLP initialized successfully tá»« {jar_path}")
    
    def _test_vncorenlp(self):
        """Test VnCoreNLP hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng"""
        test_text = "Xin chÃ o cÃ¡c báº¡n"
        try:
            result = self._run_vncorenlp(test_text)
            if not result:
                raise Exception("VnCoreNLP khÃ´ng tráº£ vá» káº¿t quáº£")
        except Exception as e:
            raise Exception(f"VnCoreNLP test failed: {e}")
    
    def _run_vncorenlp(self, text: str) -> List[List[str]]:
        """
        Cháº¡y VnCoreNLP qua command line vÃ  parse káº¿t quáº£
        
        Args:
            text: Text cáº§n phÃ¢n tÃ¡ch tá»«
            
        Returns:
            List of sentences, má»—i sentence lÃ  list of words
        """
        # Táº¡o file táº¡m cho input
        with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', suffix='.txt', delete=False) as input_file:
            input_file.write(text)
            input_file_path = input_file.name
        
        # Táº¡o file táº¡m cho output  
        output_file_path = input_file_path + '.output'
        
        try:
            # Cháº¡y VnCoreNLP
            cmd = [
                'java', self.max_heap_size, '-jar', self.jar_path,
                '-fin', input_file_path,
                '-fout', output_file_path,
                '-annotators', 'wseg'
            ]
            
            result = subprocess.run(
                cmd, 
                capture_output=True, 
                text=True, 
                timeout=30,
                encoding='utf-8'
            )
            
            if result.returncode != 0:
                raise Exception(f"VnCoreNLP failed: {result.stderr}")
            
            # Äá»c káº¿t quáº£
            if not os.path.exists(output_file_path):
                raise Exception("VnCoreNLP output file khÃ´ng Ä‘Æ°á»£c táº¡o")
            
            return self._parse_vncorenlp_output(output_file_path)
        
        finally:
            # Cleanup files
            for file_path in [input_file_path, output_file_path]:
                if os.path.exists(file_path):
                    os.unlink(file_path)
    
    def _parse_vncorenlp_output(self, output_file_path: str) -> List[List[str]]:
        """Parse VnCoreNLP output file thÃ nh list of sentences"""
        sentences = []
        current_sentence = []
        
        with open(output_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    if current_sentence:
                        sentences.append(current_sentence)
                        current_sentence = []
                    continue
                
                # Parse line format: word_index word pos_tag ner_tag head dep_rel
                parts = line.split('\t')
                if len(parts) >= 2:
                    word = parts[1]  # Word á»Ÿ column thá»© 2
                    current_sentence.append(word)
        
        # Add last sentence if exists
        if current_sentence:
            sentences.append(current_sentence)
        
        return sentences
    
    def tokenize(self, text: str) -> List[List[str]]:
        """
        Tokenize text thÃ nh sentences cá»§a words
        
        Args:
            text: Input text
            
        Returns:
            List of sentences, má»—i sentence lÃ  list of words
        """
        if not text or not text.strip():
            return [[]]
        
        return self._run_vncorenlp(text.strip())

class PhoBERTPreprocessor:
    def __init__(self, model_name="vinai/phobert-base", max_length=256, vncorenlp_jar_path=None):
        """
        Khá»Ÿi táº¡o preprocessor cho PhoBERT vá»›i VnCoreNLP local
        
        Args:
            model_name: "vinai/phobert-base" hoáº·c "vinai/phobert-large"
            max_length: Äá»™ dÃ i tá»‘i Ä‘a cá»§a sequence
            vncorenlp_jar_path: ÄÆ°á»ng dáº«n Ä‘áº¿n VnCoreNLP jar file
        """
        self.model_name = model_name
        self.max_length = max_length
        
        print(f"ğŸš€ Äang khá»Ÿi táº¡o {model_name}...")
        
        # Khá»Ÿi táº¡o tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        
        # Khá»Ÿi táº¡o VnCoreNLP local
        self.setup_local_vncorenlp(vncorenlp_jar_path)
        
        # Label encoder
        self.label_encoder = LabelEncoder()
        
        print("âœ… Khá»Ÿi táº¡o hoÃ n thÃ nh!")
    
    def setup_local_vncorenlp(self, vncorenlp_jar_path):
        """Setup VnCoreNLP tá»« jar file local"""
        try:
            if not vncorenlp_jar_path:
                # Default path trong project
                project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                vncorenlp_jar_path = os.path.join(project_root, 'VnCoreNLP', 'VnCoreNLP-1.1.1.jar')
            
            print(f"ğŸ”„ Äang setup VnCoreNLP tá»«: {vncorenlp_jar_path}")
            
            self.segmenter = LocalVnCoreNLP(
                jar_path=vncorenlp_jar_path,
                max_heap_size='-Xmx1g'
            )
            
            print("âœ… VnCoreNLP local Ä‘Ã£ sáºµn sÃ ng!")
            
        except Exception as e:
            print(f"âŒ Lá»—i khá»Ÿi táº¡o VnCoreNLP: {e}")
            print("ğŸ’¡ HÆ°á»›ng dáº«n:")
            print("   1. Äáº£m báº£o Java Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t: java -version")
            print("   2. Äáº£m báº£o file VnCoreNLP-1.1.1.jar tá»“n táº¡i")
            print(f"   3. Path hiá»‡n táº¡i: {vncorenlp_jar_path}")
            self.segmenter = None
    
    def word_segment(self, text: str) -> str:
        """
        Word segmentation sá»­ dá»¥ng VnCoreNLP local
        """
        if not self.segmenter:
            return text
        
        try:
            # VnCoreNLP word segmentation
            segmented = self.segmenter.tokenize(text)
            
            # Flatten sentences thÃ nh single string
            result = []
            for sentence in segmented:
                result.extend(sentence)
            
            return " ".join(result)
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i word segmentation: {e}")
            return text
    
    def preprocess_text(self, text: str) -> str:
        """
        Tiá»n xá»­ lÃ½ text cho PhoBERT
        """
        # LÃ m sáº¡ch text cÆ¡ báº£n
        text = text.strip()
        if not text:
            return ""
        
        # Word segmentation
        segmented_text = self.word_segment(text)
        
        return segmented_text
    
    def encode_texts(self, texts: List[str]) -> Dict:
        """
        Encode danh sÃ¡ch texts thÃ nh format cho PhoBERT
        """
        print(f"ğŸ”„ Äang encode {len(texts)} texts...")
        
        # Tiá»n xá»­ lÃ½ texts vá»›i progress bar
        processed_texts = []
        for text in tqdm(texts, desc="VnCoreNLP word segmentation"):
            processed_text = self.preprocess_text(text)
            processed_texts.append(processed_text)
        
        # Tokenize vá»›i PhoBERT tokenizer
        print("ğŸ”„ Äang tokenize vá»›i PhoBERT...")
        encoded = self.tokenizer(
            processed_texts,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'processed_texts': processed_texts
        }
    
    def encode_labels(self, labels: List[str]) -> Dict:
        """
        Encode labels
        """
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        return {
            'labels': torch.tensor(encoded_labels, dtype=torch.long),
            'label_mapping': {label: idx for idx, label in enumerate(self.label_encoder.classes_)},
            'num_classes': len(self.label_encoder.classes_)
        }
    
    def load_data(self, file_path: str) -> Tuple[List[str], List[str]]:
        """
        Load dá»¯ liá»‡u tá»« CSV hoáº·c JSONL
        """
        texts = []
        labels = []
        
        if file_path.endswith('.csv'):
            with open(file_path, 'r', encoding='utf-8') as f:
                reader = csv.DictReader(f)
                for row in reader:
                    texts.append(row['title'])
                    labels.append(row['label'])
        elif file_path.endswith('.jsonl'):
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    data = json.loads(line.strip())
                    texts.append(data['title'])
                    labels.append(data['label'])
        
        return texts, labels
    
    def save_processed_data(self, data: Dict, output_path: str):
        """
        LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
        """
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'wb') as f:
            pickle.dump(data, f)
        
        print(f"âœ… ÄÃ£ lÆ°u: {output_path}")
    
    def process_dataset(self, input_dir: str, output_dir: str):
        """
        Xá»­ lÃ½ toÃ n bá»™ dataset
        """
        print(f"=" * 60)
        print(f"ğŸ“Š TIá»€N Xá»¬ LÃ Dá»® LIá»†U CHO {self.model_name.upper()}")
        print(f"ğŸ“ Max length: {self.max_length}")
        print(f"ğŸ”§ VnCoreNLP: Local JAR file")
        print(f"=" * 60)
        
        # Táº¡o thÆ° má»¥c output
        os.makedirs(output_dir, exist_ok=True)
        
        splits = ['train', 'val', 'test']
        all_labels = []
        
        # Thu tháº­p táº¥t cáº£ labels Ä‘á»ƒ fit label encoder
        print("ğŸ”„ Äang thu tháº­p labels...")
        for split in splits:
            csv_path = os.path.join(input_dir, split, f'{split}.csv')
            if os.path.exists(csv_path):
                _, labels = self.load_data(csv_path)
                all_labels.extend(labels)
        
        # Fit label encoder
        self.label_encoder.fit(all_labels)
        label_info = {
            'label_mapping': {label: idx for idx, label in enumerate(self.label_encoder.classes_)},
            'num_classes': len(self.label_encoder.classes_)
        }
        
        print(f"ğŸ“‹ Label mapping: {label_info['label_mapping']}")
        print()
        
        # Xá»­ lÃ½ tá»«ng split
        for split in splits:
            print(f"ğŸ”„ Äang xá»­ lÃ½ {split} set...")
            
            csv_path = os.path.join(input_dir, split, f'{split}.csv')
            if not os.path.exists(csv_path):
                print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y {csv_path}")
                continue
            
            # Load dá»¯ liá»‡u
            texts, labels = self.load_data(csv_path)
            print(f"ğŸ“Š {split}: {len(texts)} samples")
            
            # Encode texts
            encoded_texts = self.encode_texts(texts)
            
            # Encode labels
            encoded_labels = self.label_encoder.transform(labels)
            
            # Táº¡o dataset
            dataset = {
                'input_ids': encoded_texts['input_ids'],
                'attention_mask': encoded_texts['attention_mask'],
                'labels': torch.tensor(encoded_labels, dtype=torch.long),
                'original_texts': texts,
                'processed_texts': encoded_texts['processed_texts'],
                'original_labels': labels,
                'label_mapping': label_info['label_mapping'],
                'num_classes': label_info['num_classes']
            }
            
            # LÆ°u dá»¯ liá»‡u
            output_path = os.path.join(output_dir, f'{split}_processed.pkl')
            self.save_processed_data(dataset, output_path)
            
            # Thá»‘ng kÃª
            stats = {
                'split': split,
                'num_samples': len(texts),
                'max_length': self.max_length,
                'model_name': self.model_name,
                'avg_length': np.mean([len(text.split()) for text in encoded_texts['processed_texts']]),
                'label_distribution': {label: labels.count(label) for label in set(labels)},
                'vncorenlp_method': 'Local JAR file'
            }
            
            stats_path = os.path.join(output_dir, f'{split}_stats.json')
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… HoÃ n thÃ nh {split} set")
            print()
        
        # LÆ°u thÃ´ng tin preprocessing
        info = {
            'model_name': self.model_name,
            'max_length': self.max_length,
            'tokenizer_vocab_size': self.tokenizer.vocab_size,
            'label_mapping': label_info['label_mapping'],
            'num_classes': label_info['num_classes'],
            'preprocessing_info': {
                'word_segmentation': 'VnCoreNLP Local JAR',
                'vncorenlp_path': self.segmenter.jar_path if self.segmenter else None,
                'special_tokens': self.tokenizer.special_tokens_map
            }
        }
        
        info_path = os.path.join(output_dir, 'preprocessing_info.json')
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        
        print("ğŸ‰ HoÃ n thÃ nh tiá»n xá»­ lÃ½ dá»¯ liá»‡u!")
        print(f"ğŸ“ Dá»¯ liá»‡u Ä‘Ã£ lÆ°u táº¡i: {output_dir}")

def main():
    """
    Main function - Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n á»Ÿ Ä‘Ã¢y
    """
    print("ğŸš€ PhoBERT Preprocessing vá»›i VnCoreNLP Local")
    print()
    
    # Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
    INPUT_DIR = "simple_dataset"  # ThÆ° má»¥c chá»©a dá»¯ liá»‡u gá»‘c
    
    # ÄÆ°á»ng dáº«n VnCoreNLP jar file (tá»± Ä‘á»™ng detect tá»« project structure)
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    VNCORENLP_JAR = os.path.join(project_root, 'VnCoreNLP', 'VnCoreNLP-1.1.1.jar')
    
    print(f"ğŸ“‚ Input directory: {INPUT_DIR}")
    print(f"ğŸ”§ VnCoreNLP JAR: {VNCORENLP_JAR}")
    
    if not os.path.exists(VNCORENLP_JAR):
        print(f"âŒ VnCoreNLP jar file khÃ´ng tÃ¬m tháº¥y: {VNCORENLP_JAR}")
        print("ğŸ’¡ HÃ£y Ä‘áº£m báº£o file VnCoreNLP-1.1.1.jar tá»“n táº¡i!")
        return
    
    print()
    
    # Xá»­ lÃ½ cho PhoBERT-base
    print("=" * 60)
    print("ğŸ“Š PHOBERT-BASE vá»›i VnCoreNLP Local")
    print("=" * 60)
    
    try:
        processor_base = PhoBERTPreprocessor(
            model_name="vinai/phobert-base",
            max_length=256,
            vncorenlp_jar_path=VNCORENLP_JAR
        )
        processor_base.process_dataset(INPUT_DIR, "data-bert-local/phobert-base")
        
    except Exception as e:
        print(f"âŒ Lá»—i xá»­ lÃ½ PhoBERT-base: {e}")
        return
    
    print("\n" + "="*60 + "\n")
    
    # Xá»­ lÃ½ cho PhoBERT-large  
    print("=" * 60)
    print("ğŸ“Š PHOBERT-LARGE vá»›i VnCoreNLP Local")
    print("=" * 60)
    
    try:
        processor_large = PhoBERTPreprocessor(
            model_name="vinai/phobert-large", 
            max_length=256,
            vncorenlp_jar_path=VNCORENLP_JAR
        )
        processor_large.process_dataset(INPUT_DIR, "data-bert-local/phobert-large")
        
    except Exception as e:
        print(f"âŒ Lá»—i xá»­ lÃ½ PhoBERT-large: {e}")
        return
    
    print("\n" + "="*60)
    print("ğŸ‰ HOÃ€N THÃ€NH Táº¤T Cáº¢!")
    print("ğŸ“ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong data-bert-local/")
    print("ğŸš€ BÃ¢y giá» cÃ³ thá»ƒ cháº¡y training script!")

if __name__ == "__main__":
    main() 