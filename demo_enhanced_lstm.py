#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Demo script cho Enhanced LSTM v·ªõi pre-trained embeddings
Load model ƒë√£ train v√† test v·ªõi text m·ªõi
"""

import os
import pickle
import torch
import json
import argparse
from typing import List, Dict
import numpy as np

# Import t·ª´ train script
from train_lstm_enhanced import EnhancedLSTM, EnhancedVietnameseTextProcessor

class EnhancedLSTMPredictor:
    """Predictor class cho Enhanced LSTM model"""
    
    def __init__(self, model_dir: str):
        self.model_dir = model_dir
        self.model = None
        self.processor = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.label_names = ['non-clickbait', 'clickbait']
        
        self.load_model()
    
    def load_model(self):
        """Load trained model v√† text processor"""
        print(f"üì• Loading Enhanced LSTM model t·ª´ {self.model_dir}")
        
        # Load text processor
        processor_path = os.path.join(self.model_dir, 'text_processor_enhanced.pkl')
        if not os.path.exists(processor_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y text processor: {processor_path}")
        
        with open(processor_path, 'rb') as f:
            self.processor = pickle.load(f)
        
        print(f"‚úÖ Loaded text processor:")
        print(f"   Vocab size: {self.processor.vocab_size}")
        print(f"   Embedding dim: {self.processor.embedding_dim}")
        print(f"   Max length: {self.processor.max_length}")
        print(f"   Embedding type: {self.processor.embedding_type}")
        
        # Load model architecture t·ª´ training history
        history_path = os.path.join(self.model_dir, 'training_history_enhanced.json')
        if os.path.exists(history_path):
            with open(history_path, 'r') as f:
                history = json.load(f)
            print(f"‚úÖ Best validation F1: {history.get('best_val_f1', 0):.4f}")
        
        # Create model
        self.model = EnhancedLSTM(
            vocab_size=self.processor.vocab_size,
            embedding_dim=self.processor.embedding_dim,
            hidden_dim=256,  # Default values
            num_layers=2,
            dropout=0.5,
            num_classes=2,
            embedding_matrix=self.processor.embedding_matrix
        )
        
        # Load model weights
        model_path = os.path.join(self.model_dir, 'best_model_enhanced.pth')
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y model weights: {model_path}")
        
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()
        
        print(f"‚úÖ Model loaded successfully!")
        print(f"   Device: {self.device}")
    
    def predict_single(self, text: str) -> Dict:
        """Predict cho m·ªôt text"""
        # Preprocess text
        sequence = self.processor.text_to_sequence(text)
        
        # Convert to tensor
        input_tensor = torch.tensor([sequence], dtype=torch.long).to(self.device)
        attention_mask = (input_tensor != 0).long()
        
        # Predict
        with torch.no_grad():
            outputs = self.model(input_tensor, attention_mask)
            probabilities = torch.softmax(outputs, dim=1)
            predicted_class = torch.argmax(outputs, dim=1).item()
            confidence = probabilities[0][predicted_class].item()
        
        result = {
            'text': text,
            'predicted_label': self.label_names[predicted_class],
            'predicted_class': predicted_class,
            'confidence': confidence,
            'probabilities': {
                'non-clickbait': probabilities[0][0].item(),
                'clickbait': probabilities[0][1].item()
            },
            'sequence_length': len(sequence)
        }
        
        return result
    
    def predict_batch(self, texts: List[str]) -> List[Dict]:
        """Predict cho nhi·ªÅu texts"""
        results = []
        
        print(f"üîÑ Predicting {len(texts)} texts...")
        
        for i, text in enumerate(texts):
            result = self.predict_single(text)
            results.append(result)
            
            if (i + 1) % 10 == 0:
                print(f"   Processed {i + 1}/{len(texts)}")
        
        return results
    
    def analyze_predictions(self, results: List[Dict]):
        """Ph√¢n t√≠ch k·∫øt qu·∫£ predictions"""
        print("\n" + "="*60)
        print("üìä PH√ÇN T√çCH K·∫æT QU·∫¢ PREDICTIONS")
        print("="*60)
        
        clickbait_count = sum(1 for r in results if r['predicted_class'] == 1)
        non_clickbait_count = len(results) - clickbait_count
        
        avg_confidence = np.mean([r['confidence'] for r in results])
        
        print(f"üìà T·ªïng quan:")
        print(f"   Total texts: {len(results)}")
        print(f"   Clickbait: {clickbait_count} ({clickbait_count/len(results)*100:.1f}%)")
        print(f"   Non-clickbait: {non_clickbait_count} ({non_clickbait_count/len(results)*100:.1f}%)")
        print(f"   Average confidence: {avg_confidence:.3f}")
        
        # High confidence predictions
        high_conf_threshold = 0.8
        high_conf_results = [r for r in results if r['confidence'] > high_conf_threshold]
        
        print(f"\nüéØ High confidence predictions (>{high_conf_threshold}):")
        print(f"   Count: {len(high_conf_results)}/{len(results)} ({len(high_conf_results)/len(results)*100:.1f}%)")
        
        if high_conf_results:
            print("\nüìã Top confident predictions:")
            sorted_results = sorted(results, key=lambda x: x['confidence'], reverse=True)
            
            for i, result in enumerate(sorted_results[:5]):
                print(f"\n{i+1}. [{result['predicted_label'].upper()}] (confidence: {result['confidence']:.3f})")
                print(f"   Text: {result['text'][:100]}{'...' if len(result['text']) > 100 else ''}")

def demo_interactive(predictor: EnhancedLSTMPredictor):
    """Demo t∆∞∆°ng t√°c"""
    print("\n" + "="*60)
    print("üéÆ INTERACTIVE DEMO - Enhanced LSTM")
    print("="*60)
    print("Nh·∫≠p text ƒë·ªÉ ph√¢n lo·∫°i clickbait (g√µ 'quit' ƒë·ªÉ tho√°t)")
    print()
    
    while True:
        text = input("üìù Nh·∫≠p text: ").strip()
        
        if text.lower() in ['quit', 'exit', 'q']:
            print("üëã T·∫°m bi·ªát!")
            break
        
        if not text:
            continue
        
        result = predictor.predict_single(text)
        
        print(f"\nüìä K·∫øt qu·∫£:")
        print(f"   Label: {result['predicted_label'].upper()}")
        print(f"   Confidence: {result['confidence']:.3f}")
        print(f"   Probabilities:")
        print(f"     Non-clickbait: {result['probabilities']['non-clickbait']:.3f}")
        print(f"     Clickbait: {result['probabilities']['clickbait']:.3f}")
        print(f"   Sequence length: {result['sequence_length']}")
        
        # Interpretation
        if result['confidence'] > 0.8:
            conf_level = "R·∫§T TIN C·∫¨Y"
        elif result['confidence'] > 0.6:
            conf_level = "TIN C·∫¨Y"
        else:
            conf_level = "KH√îNG CH·∫ÆC CH·∫ÆN"
        
        print(f"   ƒê√°nh gi√°: {conf_level}")
        print()

def demo_predefined_examples(predictor: EnhancedLSTMPredictor):
    """Demo v·ªõi examples c√≥ s·∫µn"""
    print("\n" + "="*60)
    print("üìã DEMO V·ªöI EXAMPLES C√ì S·∫¥N")
    print("="*60)
    
    examples = [
        # Clickbait examples
        "B·∫°n s·∫Ω kh√¥ng th·ªÉ tin ƒë∆∞·ª£c ƒëi·ªÅu g√¨ x·∫£y ra ti·∫øp theo!",
        "10 b√≠ m·∫≠t m√† c√°c b√°c sƒ© kh√¥ng mu·ªën b·∫°n bi·∫øt",
        "C√°ch ki·∫øm ti·ªÅn online 100 tri·ªáu/th√°ng ch·ªâ v·ªõi 1 click",
        "Sao Vi·ªát b·∫•t ng·ªù ti·∫øt l·ªô b√≠ m·∫≠t g√¢y s·ªëc",
        "Th·ª±c ph·∫©m n√†y c√≥ th·ªÉ gi·∫øt b·∫°n - b·∫°n ƒÉn m·ªói ng√†y!",
        
        # Non-clickbait examples  
        "Ch√≠nh ph·ªß c√¥ng b·ªë ch√≠nh s√°ch m·ªõi v·ªÅ thu·∫ø thu nh·∫≠p c√° nh√¢n",
        "Nghi√™n c·ª©u m·ªõi v·ªÅ t√°c ƒë·ªông c·ªßa bi·∫øn ƒë·ªïi kh√≠ h·∫≠u ƒë·∫øn n√¥ng nghi·ªáp",
        "ƒê·∫°i h·ªçc Qu·ªëc gia H√† N·ªôi tuy·ªÉn sinh nƒÉm 2024",
        "T·ª∑ gi√° USD/VND h√¥m nay ·ªïn ƒë·ªãnh ·ªü m·ª©c 24.000",
        "WHO khuy·∫øn c√°o v·ªÅ ph√≤ng ch·ªëng d·ªãch b·ªánh m√πa ƒë√¥ng"
    ]
    
    results = predictor.predict_batch(examples)
    
    print("\nüìä K·∫æT QU·∫¢ CHI TI·∫æT:")
    print("-" * 80)
    
    for i, result in enumerate(results):
        status = "‚úÖ" if result['predicted_label'] == 'clickbait' and i < 5 else "‚úÖ" if result['predicted_label'] == 'non-clickbait' and i >= 5 else "‚ùå"
        
        print(f"\n{i+1}. {status} [{result['predicted_label'].upper()}] (confidence: {result['confidence']:.3f})")
        print(f"   Text: {result['text']}")
        print(f"   Probabilities: Non-clickbait: {result['probabilities']['non-clickbait']:.3f}, "
              f"Clickbait: {result['probabilities']['clickbait']:.3f}")
    
    # Analyze results
    predictor.analyze_predictions(results)

def main():
    parser = argparse.ArgumentParser(description="Enhanced LSTM Demo")
    parser.add_argument('--model_dir', type=str, required=True,
                       help='Directory ch·ª©a trained model')
    parser.add_argument('--mode', type=str, default='interactive',
                       choices=['interactive', 'examples', 'both'],
                       help='Demo mode')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.model_dir):
        print(f"‚ùå Model directory kh√¥ng t·ªìn t·∫°i: {args.model_dir}")
        return
    
    print("=== ENHANCED LSTM DEMO ===")
    print()
    
    # Load predictor
    try:
        predictor = EnhancedLSTMPredictor(args.model_dir)
    except Exception as e:
        print(f"‚ùå L·ªói loading model: {e}")
        return
    
    if args.mode in ['examples', 'both']:
        demo_predefined_examples(predictor)
    
    if args.mode in ['interactive', 'both']:
        demo_interactive(predictor)

if __name__ == "__main__":
    main() 