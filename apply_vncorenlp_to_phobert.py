#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Ãp dá»¥ng VnCoreNLP cho PhoBERT Data Preprocessing
Script nÃ y sá»­ dá»¥ng VnCoreNLP package Ä‘á»ƒ tiá»n xá»­ lÃ½ dá»¯ liá»‡u cho PhoBERT-base vÃ  PhoBERT-large
"""

import os
import json
import pickle
import csv
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from tqdm import tqdm

try:
    from transformers import AutoTokenizer
    from vncorenlp import VnCoreNLP
    from sklearn.preprocessing import LabelEncoder
    import torch
    print("âœ… Import packages thÃ nh cÃ´ng!")
except ImportError as e:
    print(f"âŒ Lá»—i import: {e}")
    print("ğŸ’¡ CÃ i Ä‘áº·t packages cáº§n thiáº¿t:")
    print("pip install torch transformers scikit-learn vncorenlp")
    exit(1)

class VnCoreNLPPhoBERTPreprocessor:
    """
    Preprocessor cho PhoBERT sá»­ dá»¥ng VnCoreNLP package
    """
    
    def __init__(self, model_name="vinai/phobert-base", max_length=256, vncorenlp_jar_path=None):
        """
        Khá»Ÿi táº¡o preprocessor
        
        Args:
            model_name: "vinai/phobert-base" hoáº·c "vinai/phobert-large"
            max_length: Äá»™ dÃ i tá»‘i Ä‘a cá»§a sequence
            vncorenlp_jar_path: ÄÆ°á»ng dáº«n Ä‘áº¿n VnCoreNLP jar file
        """
        self.model_name = model_name
        self.max_length = max_length
        
        print(f"ğŸš€ Khá»Ÿi táº¡o {model_name} vá»›i VnCoreNLP...")
        
        # Khá»Ÿi táº¡o PhoBERT tokenizer
        print("ğŸ“ Äang táº£i PhoBERT tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        
        # Khá»Ÿi táº¡o VnCoreNLP
        self.setup_vncorenlp(vncorenlp_jar_path)
        
        # Label encoder
        self.label_encoder = LabelEncoder()
        
        print("âœ… Khá»Ÿi táº¡o hoÃ n thÃ nh!")
        
    def setup_vncorenlp(self, vncorenlp_jar_path=None):
        """Setup VnCoreNLP tá»« package Ä‘Ã£ cÃ i Ä‘áº·t"""
        try:
            if not vncorenlp_jar_path:
                # Sá»­ dá»¥ng VnCoreNLP tá»« project
                vncorenlp_jar_path = os.path.join(os.getcwd(), 'VnCoreNLP', 'VnCoreNLP-1.2.jar')
            
            print(f"ğŸ”§ Äang khá»Ÿi táº¡o VnCoreNLP tá»«: {vncorenlp_jar_path}")
            
            if not os.path.exists(vncorenlp_jar_path):
                raise FileNotFoundError(f"VnCoreNLP jar file khÃ´ng tÃ¬m tháº¥y: {vncorenlp_jar_path}")
            
            # Khá»Ÿi táº¡o VnCoreNLP vá»›i word segmentation
            self.vncorenlp = VnCoreNLP(
                vncorenlp_jar_path, 
                annotators="wseg", 
                max_heap_size='-Xmx2g'
            )
            
            print("âœ… VnCoreNLP Ä‘Ã£ sáºµn sÃ ng!")
            
            # Test VnCoreNLP
            test_text = "ÄÃ¢y lÃ  test VnCoreNLP."
            test_result = self.vncorenlp.annotate(test_text)
            print(f"ğŸ§ª Test VnCoreNLP: {test_result}")
            
        except Exception as e:
            print(f"âŒ Lá»—i khá»Ÿi táº¡o VnCoreNLP: {e}")
            print("ğŸ’¡ Kiá»ƒm tra:")
            print("1. Java Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t: java -version")
            print("2. VnCoreNLP jar file tá»“n táº¡i")
            print("3. Äá»§ RAM (>= 2GB)")
            raise
    
    def word_segment(self, text: str) -> str:
        """
        Word segmentation sá»­ dá»¥ng VnCoreNLP package
        """
        try:
            if not text or not text.strip():
                return ""
            
            # VnCoreNLP word segmentation
            result = self.vncorenlp.annotate(text.strip())
            
            # Extract segmented words
            segmented_words = []
            for sentence in result['sentences']:
                for word in sentence:
                    segmented_words.append(word['form'])
            
            return " ".join(segmented_words)
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i word segmentation cho text: '{text[:50]}...': {e}")
            return text  # Fallback vá» text gá»‘c
    
    def preprocess_text(self, text: str) -> str:
        """
        Tiá»n xá»­ lÃ½ text cho PhoBERT
        """
        # LÃ m sáº¡ch text cÆ¡ báº£n
        text = text.strip()
        if not text:
            return ""
        
        # Word segmentation vá»›i VnCoreNLP
        segmented_text = self.word_segment(text)
        
        return segmented_text
    
    def encode_texts(self, texts: List[str]) -> Dict:
        """
        Encode danh sÃ¡ch texts thÃ nh format cho PhoBERT
        """
        print(f"ğŸ”„ Äang encode {len(texts)} texts...")
        
        # Tiá»n xá»­ lÃ½ texts vá»›i VnCoreNLP
        processed_texts = []
        print("ğŸ”¤ Äang thá»±c hiá»‡n word segmentation...")
        
        for text in tqdm(texts, desc="VnCoreNLP word segmentation"):
            processed_text = self.preprocess_text(text)
            processed_texts.append(processed_text)
        
        # Tokenize vá»›i PhoBERT tokenizer
        print("ğŸ¤– Äang tokenize vá»›i PhoBERT...")
        encoded = self.tokenizer(
            processed_texts,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        # Thá»‘ng kÃª
        print(f"ğŸ“Š Thá»‘ng kÃª encoding:")
        print(f"   - Sá»‘ texts: {len(texts)}")
        print(f"   - Max length: {self.max_length}")
        print(f"   - Input IDs shape: {encoded['input_ids'].shape}")
        print(f"   - Attention mask shape: {encoded['attention_mask'].shape}")
        
        return {
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'processed_texts': processed_texts,
            'original_texts': texts
        }
    
    def encode_labels(self, labels: List[str]) -> Dict:
        """
        Encode labels
        """
        print(f"ğŸ·ï¸ Äang encode {len(labels)} labels...")
        
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        # Thá»‘ng kÃª labels
        unique_labels, counts = np.unique(labels, return_counts=True)
        print(f"ğŸ“Š PhÃ¢n bá»‘ labels:")
        for label, count in zip(unique_labels, counts):
            percentage = (count / len(labels)) * 100
            print(f"   - {label}: {count} ({percentage:.1f}%)")
        
        return {
            'labels': torch.tensor(encoded_labels, dtype=torch.long),
            'label_mapping': {label: idx for idx, label in enumerate(self.label_encoder.classes_)},
            'num_classes': len(self.label_encoder.classes_),
            'original_labels': labels
        }
    
    def load_data_from_csv(self, file_path: str) -> Tuple[List[str], List[str]]:
        """
        Load dá»¯ liá»‡u tá»« CSV file
        """
        print(f"ğŸ“‚ Äang load dá»¯ liá»‡u tá»«: {file_path}")
        
        texts = []
        labels = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                texts.append(row['title'])
                labels.append(row['label'])
        
        print(f"âœ… ÄÃ£ load {len(texts)} samples")
        return texts, labels
    
    def save_processed_data(self, data: Dict, output_path: str, info: Dict = None):
        """
        LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
        """
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ThÃªm thÃ´ng tin preprocessing
        data['preprocessing_info'] = {
            'model_name': self.model_name,
            'max_length': self.max_length,
            'tokenizer_vocab_size': self.tokenizer.vocab_size,
            'word_segmentation': 'VnCoreNLP Package',
            'vncorenlp_version': '1.2',
            'special_tokens': {
                'bos_token': self.tokenizer.bos_token,
                'eos_token': self.tokenizer.eos_token,
                'unk_token': self.tokenizer.unk_token,
                'sep_token': self.tokenizer.sep_token,
                'pad_token': self.tokenizer.pad_token,
                'cls_token': self.tokenizer.cls_token,
                'mask_token': self.tokenizer.mask_token,
            }
        }
        
        if info:
            data['preprocessing_info'].update(info)
        
        # LÆ°u pickle file
        with open(output_path, 'wb') as f:
            pickle.dump(data, f)
        
        # LÆ°u thÃ´ng tin preprocessing dÆ°á»›i dáº¡ng JSON
        info_path = output_path.replace('.pkl', '_info.json')
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(data['preprocessing_info'], f, indent=2, ensure_ascii=False)
        
        print(f"âœ… ÄÃ£ lÆ°u: {output_path}")
        print(f"ğŸ“‹ ThÃ´ng tin: {info_path}")
    
    def process_split(self, input_dir: str, split: str, output_dir: str, label_encoder_fitted=False):
        """
        Xá»­ lÃ½ má»™t split (train/val/test)
        """
        print(f"\nğŸ“ Xá»­ lÃ½ {split.upper()}...")
        
        # Load dá»¯ liá»‡u
        csv_path = os.path.join(input_dir, split, f'{split}.csv')
        
        if not os.path.exists(csv_path):
            print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y {csv_path}")
            return None
        
        texts, labels = self.load_data_from_csv(csv_path)
        
        # Encode texts
        encoded_texts = self.encode_texts(texts)
        
        # Encode labels
        if not label_encoder_fitted:
            encoded_labels = self.encode_labels(labels)
        else:
            # Sá»­ dá»¥ng label encoder Ä‘Ã£ fit tá»« trÆ°á»›c
            encoded_label_values = self.label_encoder.transform(labels)
            encoded_labels = {
                'labels': torch.tensor(encoded_label_values, dtype=torch.long),
                'label_mapping': {label: idx for idx, label in enumerate(self.label_encoder.classes_)},
                'num_classes': len(self.label_encoder.classes_),
                'original_labels': labels
            }
        
        # Combine data
        processed_data = {
            **encoded_texts,
            **encoded_labels
        }
        
        # LÆ°u dá»¯ liá»‡u
        output_path = os.path.join(output_dir, f'{split}_processed.pkl')
        self.save_processed_data(processed_data, output_path)
        
        return processed_data
    
    def process_dataset(self, input_dir: str, output_dir: str):
        """
        Xá»­ lÃ½ toÃ n bá»™ dataset
        """
        print(f"=" * 70)
        print(f"ğŸ¯ TIá»€N Xá»¬ LÃ Dá»® LIá»†U CHO {self.model_name.upper()}")
        print(f"ğŸ“ Max length: {self.max_length}")
        print(f"ğŸ”§ VnCoreNLP: Package version vá»›i JAR file")
        print(f"ğŸ“‚ Input: {input_dir}")
        print(f"ğŸ“‚ Output: {output_dir}")
        print(f"=" * 70)
        
        # Táº¡o thÆ° má»¥c output
        os.makedirs(output_dir, exist_ok=True)
        
        # CÃ¡c splits cáº§n xá»­ lÃ½
        splits = ['train', 'val', 'test']
        
        # Thu tháº­p táº¥t cáº£ labels Ä‘á»ƒ fit label encoder
        print("ğŸ”„ Thu tháº­p labels tá»« táº¥t cáº£ splits...")
        all_labels = []
        for split in splits:
            csv_path = os.path.join(input_dir, split, f'{split}.csv')
            if os.path.exists(csv_path):
                _, labels = self.load_data_from_csv(csv_path)
                all_labels.extend(labels)
        
        # Fit label encoder vá»›i táº¥t cáº£ labels
        print("ğŸ·ï¸ Äang fit label encoder...")
        self.label_encoder.fit(all_labels)
        label_mapping = {label: idx for idx, label in enumerate(self.label_encoder.classes_)}
        print(f"ğŸ“‹ Label mapping: {label_mapping}")
        
        # Xá»­ lÃ½ tá»«ng split
        results = {}
        for i, split in enumerate(splits):
            label_encoder_fitted = (i > 0)  # Chá»‰ fit á»Ÿ split Ä‘áº§u tiÃªn
            result = self.process_split(input_dir, split, output_dir, label_encoder_fitted)
            if result:
                results[split] = result
        
        # LÆ°u thá»‘ng kÃª tá»•ng há»£p
        summary = {
            'model_name': self.model_name,
            'max_length': self.max_length,
            'label_mapping': label_mapping,
            'num_classes': len(self.label_encoder.classes_),
            'splits_processed': list(results.keys()),
            'total_samples': sum(len(results[split]['labels']) for split in results),
            'split_sizes': {split: len(results[split]['labels']) for split in results}
        }
        
        summary_path = os.path.join(output_dir, 'processing_summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\n" + "=" * 70)
        print("âœ… HOÃ€N THÃ€NH TIá»€N Xá»¬ LÃ Dá»® LIá»†U!")
        print(f"ğŸ“Š Tá»•ng káº¿t:")
        print(f"   - Model: {self.model_name}")
        print(f"   - Splits processed: {results.keys()}")
        print(f"   - Total samples: {summary['total_samples']}")
        print(f"   - Output directory: {output_dir}")
        print(f"   - Summary: {summary_path}")
        print("=" * 70)
        
        return results

def main():
    """
    Main function Ä‘á»ƒ cháº¡y preprocessing
    """
    print("ğŸ‡»ğŸ‡³ VnCoreNLP + PhoBERT Data Preprocessing")
    print("=" * 50)
    
    # Cáº¥u hÃ¬nh
    INPUT_DIR = "simple_dataset"
    BASE_OUTPUT_DIR = "data-vncorenlp-v2"
    
    # Kiá»ƒm tra dá»¯ liá»‡u nguá»“n
    if not os.path.exists(INPUT_DIR):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c dá»¯ liá»‡u: {INPUT_DIR}")
        print("ğŸ’¡ Äáº£m báº£o cÃ³ thÆ° má»¥c simple_dataset vá»›i train/val/test")
        return
    
    print(f"ğŸ“‚ Input directory: {INPUT_DIR}")
    print(f"ğŸ“‚ Output base directory: {BASE_OUTPUT_DIR}")
    print()
    
    # Xá»­ lÃ½ PhoBERT-base
    print("ğŸ”¥ PHáº¦N 1: PHOBERT-BASE")
    try:
        processor_base = VnCoreNLPPhoBERTPreprocessor(
            model_name="vinai/phobert-base",
            max_length=256
        )
        
        output_dir_base = os.path.join(BASE_OUTPUT_DIR, "phobert-base")
        processor_base.process_dataset(INPUT_DIR, output_dir_base)
        
    except Exception as e:
        print(f"âŒ Lá»—i xá»­ lÃ½ PhoBERT-base: {e}")
        print("ğŸ’¡ Kiá»ƒm tra láº¡i cáº¥u hÃ¬nh vÃ  dependencies")
    
    print("\n" + "="*70 + "\n")
    
    # Xá»­ lÃ½ PhoBERT-large
    print("ğŸ”¥ PHáº¦N 2: PHOBERT-LARGE")
    try:
        processor_large = VnCoreNLPPhoBERTPreprocessor(
            model_name="vinai/phobert-large",
            max_length=256
        )
        
        output_dir_large = os.path.join(BASE_OUTPUT_DIR, "phobert-large")
        processor_large.process_dataset(INPUT_DIR, output_dir_large)
        
    except Exception as e:
        print(f"âŒ Lá»—i xá»­ lÃ½ PhoBERT-large: {e}")
        print("ğŸ’¡ Kiá»ƒm tra láº¡i cáº¥u hÃ¬nh vÃ  dependencies")
    
    print("\n" + "="*70)
    print("ğŸ‰ HOÃ€N THÃ€NH Táº¤T Cáº¢!")
    print("ğŸ“ Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ vÃ  lÆ°u trong:")
    print(f"   - PhoBERT-base: {BASE_OUTPUT_DIR}/phobert-base/")
    print(f"   - PhoBERT-large: {BASE_OUTPUT_DIR}/phobert-large/")
    print("\nğŸ’¡ BÆ°á»›c tiáº¿p theo:")
    print("   python scripts/phobert/train_phobert.py --data_dir data-vncorenlp-v2")
    print("="*70)

if __name__ == "__main__":
    main() 