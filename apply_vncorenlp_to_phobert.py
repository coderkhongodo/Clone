#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
√Åp d·ª•ng VnCoreNLP cho PhoBERT Data Preprocessing
Script n√†y s·ª≠ d·ª•ng VnCoreNLP package ƒë·ªÉ ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho PhoBERT-base v√† PhoBERT-large
"""

import os
import json
import pickle
import csv
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from tqdm import tqdm

try:
    from transformers import AutoTokenizer
    from vncorenlp import VnCoreNLP
    from sklearn.preprocessing import LabelEncoder
    import torch
    print("‚úÖ Import packages th√†nh c√¥ng!")
except ImportError as e:
    print(f"‚ùå L·ªói import: {e}")
    print("üí° C√†i ƒë·∫∑t packages c·∫ßn thi·∫øt:")
    print("pip install torch transformers scikit-learn vncorenlp")
    exit(1)

class VnCoreNLPPhoBERTPreprocessor:
    """
    Preprocessor cho PhoBERT s·ª≠ d·ª•ng VnCoreNLP package
    """
    
    def __init__(self, model_name="vinai/phobert-base", max_length=256, vncorenlp_jar_path=None):
        """
        Kh·ªüi t·∫°o preprocessor
        
        Args:
            model_name: "vinai/phobert-base" ho·∫∑c "vinai/phobert-large"
            max_length: ƒê·ªô d√†i t·ªëi ƒëa c·ªßa sequence
            vncorenlp_jar_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn VnCoreNLP jar file
        """
        self.model_name = model_name
        self.max_length = max_length
        
        print(f"üöÄ Kh·ªüi t·∫°o {model_name} v·ªõi VnCoreNLP...")
        
        # Kh·ªüi t·∫°o PhoBERT tokenizer
        print("üìù ƒêang t·∫£i PhoBERT tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
        
        # Kh·ªüi t·∫°o VnCoreNLP
        self.setup_vncorenlp(vncorenlp_jar_path)
        
        # Label encoder
        self.label_encoder = LabelEncoder()
        
        print("‚úÖ Kh·ªüi t·∫°o ho√†n th√†nh!")
        
    def setup_vncorenlp(self, vncorenlp_jar_path=None):
        """Setup VnCoreNLP t·ª´ package ƒë√£ c√†i ƒë·∫∑t"""
        try:
            if not vncorenlp_jar_path:
                # S·ª≠ d·ª•ng VnCoreNLP t·ª´ project
                vncorenlp_jar_path = os.path.join(os.getcwd(), 'VnCoreNLP', 'VnCoreNLP-1.2.jar')
            
            print(f"üîß ƒêang kh·ªüi t·∫°o VnCoreNLP t·ª´: {vncorenlp_jar_path}")
            
            if not os.path.exists(vncorenlp_jar_path):
                raise FileNotFoundError(f"VnCoreNLP jar file kh√¥ng t√¨m th·∫•y: {vncorenlp_jar_path}")
            
            # Kh·ªüi t·∫°o VnCoreNLP v·ªõi word segmentation
            self.vncorenlp = VnCoreNLP(
                vncorenlp_jar_path, 
                annotators="wseg", 
                max_heap_size='-Xmx2g'
            )
            
            print("‚úÖ VnCoreNLP ƒë√£ s·∫µn s√†ng!")
            
            # Test VnCoreNLP
            test_text = "ƒê√¢y l√† test VnCoreNLP."
            test_result = self.vncorenlp.annotate(test_text)
            print(f"üß™ Test VnCoreNLP: {test_result}")
            
        except Exception as e:
            print(f"‚ùå L·ªói kh·ªüi t·∫°o VnCoreNLP: {e}")
            print("üí° Ki·ªÉm tra:")
            print("1. Java ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t: java -version")
            print("2. VnCoreNLP jar file t·ªìn t·∫°i")
            print("3. ƒê·ªß RAM (>= 2GB)")
            raise
    
    def word_segment(self, text: str) -> str:
        """
        Word segmentation s·ª≠ d·ª•ng VnCoreNLP package
        """
        try:
            if not text or not text.strip():
                return ""
            
            # VnCoreNLP word segmentation
            result = self.vncorenlp.annotate(text.strip())
            
            # Extract segmented words
            segmented_words = []
            for sentence in result['sentences']:
                for word in sentence:
                    segmented_words.append(word['form'])
            
            return " ".join(segmented_words)
            
        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói word segmentation cho text: '{text[:50]}...': {e}")
            return text  # Fallback v·ªÅ text g·ªëc
    
    def preprocess_text(self, text: str) -> str:
        """
        Ti·ªÅn x·ª≠ l√Ω text cho PhoBERT
        """
        # L√†m s·∫°ch text c∆° b·∫£n
        text = text.strip()
        if not text:
            return ""
        
        # Word segmentation v·ªõi VnCoreNLP
        segmented_text = self.word_segment(text)
        
        return segmented_text
    
    def encode_texts(self, texts: List[str]) -> Dict:
        """
        Encode danh s√°ch texts th√†nh format cho PhoBERT
        """
        print(f"üîÑ ƒêang encode {len(texts)} texts...")
        
        # Ti·ªÅn x·ª≠ l√Ω texts v·ªõi VnCoreNLP
        processed_texts = []
        print("üî§ ƒêang th·ª±c hi·ªán word segmentation...")
        
        for text in tqdm(texts, desc="VnCoreNLP word segmentation"):
            processed_text = self.preprocess_text(text)
            processed_texts.append(processed_text)
        
        # Tokenize v·ªõi PhoBERT tokenizer
        print("ü§ñ ƒêang tokenize v·ªõi PhoBERT...")
        encoded = self.tokenizer(
            processed_texts,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        # Th·ªëng k√™
        print(f"üìä Th·ªëng k√™ encoding:")
        print(f"   - S·ªë texts: {len(texts)}")
        print(f"   - Max length: {self.max_length}")
        print(f"   - Input IDs shape: {encoded['input_ids'].shape}")
        print(f"   - Attention mask shape: {encoded['attention_mask'].shape}")
        
        return {
            'input_ids': encoded['input_ids'],
            'attention_mask': encoded['attention_mask'],
            'processed_texts': processed_texts,
            'original_texts': texts
        }
    
    def encode_labels(self, labels: List[str]) -> Dict:
        """
        Encode labels
        """
        print(f"üè∑Ô∏è ƒêang encode {len(labels)} labels...")
        
        encoded_labels = self.label_encoder.fit_transform(labels)
        
        # Th·ªëng k√™ labels
        unique_labels, counts = np.unique(labels, return_counts=True)
        print(f"üìä Ph√¢n b·ªë labels:")
        for label, count in zip(unique_labels, counts):
            percentage = (count / len(labels)) * 100
            print(f"   - {label}: {count} ({percentage:.1f}%)")
        
        return {
            'labels': torch.tensor(encoded_labels, dtype=torch.long),
            'label_mapping': {label: idx for idx, label in enumerate(self.label_encoder.classes_)},
            'num_classes': len(self.label_encoder.classes_),
            'original_labels': labels
        }
    
    def load_data_from_csv(self, file_path: str) -> Tuple[List[str], List[str]]:
        """
        Load d·ªØ li·ªáu t·ª´ CSV file
        """
        print(f"üìÇ ƒêang load d·ªØ li·ªáu t·ª´: {file_path}")
        
        texts = []
        labels = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                texts.append(row['title'])
                labels.append(row['label'])
        
        print(f"‚úÖ ƒê√£ load {len(texts)} samples")
        return texts, labels
    
    def save_processed_data(self, data: Dict, output_path: str, info: Dict = None):
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
        """
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # Th√™m th√¥ng tin preprocessing
        data['preprocessing_info'] = {
            'model_name': self.model_name,
            'max_length': self.max_length,
            'tokenizer_vocab_size': self.tokenizer.vocab_size,
            'word_segmentation': 'VnCoreNLP Package',
            'vncorenlp_version': '1.2',
            'special_tokens': {
                'bos_token': self.tokenizer.bos_token,
                'eos_token': self.tokenizer.eos_token,
                'unk_token': self.tokenizer.unk_token,
                'sep_token': self.tokenizer.sep_token,
                'pad_token': self.tokenizer.pad_token,
                'cls_token': self.tokenizer.cls_token,
                'mask_token': self.tokenizer.mask_token,
            }
        }
        
        if info:
            data['preprocessing_info'].update(info)
        
        # L∆∞u pickle file
        with open(output_path, 'wb') as f:
            pickle.dump(data, f)
        
        # L∆∞u th√¥ng tin preprocessing d∆∞·ªõi d·∫°ng JSON
        info_path = output_path.replace('.pkl', '_info.json')
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(data['preprocessing_info'], f, indent=2, ensure_ascii=False)
        
        print(f"‚úÖ ƒê√£ l∆∞u: {output_path}")
        print(f"üìã Th√¥ng tin: {info_path}")
    
    def process_split(self, input_dir: str, split: str, output_dir: str, label_encoder_fitted=False):
        """
        X·ª≠ l√Ω m·ªôt split (train/val/test)
        """
        print(f"\nüìÅ X·ª≠ l√Ω {split.upper()}...")
        
        # Load d·ªØ li·ªáu
        csv_path = os.path.join(input_dir, split, f'{split}.csv')
        
        if not os.path.exists(csv_path):
            print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y {csv_path}")
            return None
        
        texts, labels = self.load_data_from_csv(csv_path)
        
        # Encode texts
        encoded_texts = self.encode_texts(texts)
        
        # Encode labels
        if not label_encoder_fitted:
            encoded_labels = self.encode_labels(labels)
        else:
            # S·ª≠ d·ª•ng label encoder ƒë√£ fit t·ª´ tr∆∞·ªõc
            encoded_label_values = self.label_encoder.transform(labels)
            encoded_labels = {
                'labels': torch.tensor(encoded_label_values, dtype=torch.long),
                'label_mapping': {label: idx for idx, label in enumerate(self.label_encoder.classes_)},
                'num_classes': len(self.label_encoder.classes_),
                'original_labels': labels
            }
        
        # Combine data
        processed_data = {
            **encoded_texts,
            **encoded_labels
        }
        
        # L∆∞u d·ªØ li·ªáu
        output_path = os.path.join(output_dir, f'{split}_processed.pkl')
        self.save_processed_data(processed_data, output_path)
        
        return processed_data
    
    def process_dataset(self, input_dir: str, output_dir: str):
        """
        X·ª≠ l√Ω to√†n b·ªô dataset
        """
        print(f"=" * 70)
        print(f"üéØ TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU CHO {self.model_name.upper()}")
        print(f"üìè Max length: {self.max_length}")
        print(f"üîß VnCoreNLP: Package version v·ªõi JAR file")
        print(f"üìÇ Input: {input_dir}")
        print(f"üìÇ Output: {output_dir}")
        print(f"=" * 70)
        
        # T·∫°o th∆∞ m·ª•c output
        os.makedirs(output_dir, exist_ok=True)
        
        # C√°c splits c·∫ßn x·ª≠ l√Ω
        splits = ['train', 'val', 'test']
        
        # Thu th·∫≠p t·∫•t c·∫£ labels ƒë·ªÉ fit label encoder
        print("üîÑ Thu th·∫≠p labels t·ª´ t·∫•t c·∫£ splits...")
        all_labels = []
        for split in splits:
            csv_path = os.path.join(input_dir, split, f'{split}.csv')
            if os.path.exists(csv_path):
                _, labels = self.load_data_from_csv(csv_path)
                all_labels.extend(labels)
        
        # Fit label encoder v·ªõi t·∫•t c·∫£ labels
        print("üè∑Ô∏è ƒêang fit label encoder...")
        self.label_encoder.fit(all_labels)
        label_mapping = {label: idx for idx, label in enumerate(self.label_encoder.classes_)}
        print(f"üìã Label mapping: {label_mapping}")
        
        # X·ª≠ l√Ω t·ª´ng split
        results = {}
        for i, split in enumerate(splits):
            label_encoder_fitted = (i > 0)  # Ch·ªâ fit ·ªü split ƒë·∫ßu ti√™n
            result = self.process_split(input_dir, split, output_dir, label_encoder_fitted)
            if result:
                results[split] = result
        
        # L∆∞u th·ªëng k√™ t·ªïng h·ª£p
        summary = {
            'model_name': self.model_name,
            'max_length': self.max_length,
            'label_mapping': label_mapping,
            'num_classes': len(self.label_encoder.classes_),
            'splits_processed': list(results.keys()),
            'total_samples': sum(len(results[split]['labels']) for split in results),
            'split_sizes': {split: len(results[split]['labels']) for split in results}
        }
        
        summary_path = os.path.join(output_dir, 'processing_summary.json')
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\n" + "=" * 70)
        print("‚úÖ HO√ÄN TH√ÄNH TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU!")
        print(f"üìä T·ªïng k·∫øt:")
        print(f"   - Model: {self.model_name}")
        print(f"   - Splits processed: {results.keys()}")
        print(f"   - Total samples: {summary['total_samples']}")
        print(f"   - Output directory: {output_dir}")
        print(f"   - Summary: {summary_path}")
        print("=" * 70)
        
        return results

def main():
    """
    Main function ƒë·ªÉ ch·∫°y preprocessing
    """
    print("üáªüá≥ VnCoreNLP + PhoBERT Data Preprocessing")
    print("=" * 50)
    
    # C·∫•u h√¨nh
    INPUT_DIR = "simple_dataset"
    BASE_OUTPUT_DIR = "data-vncorenlp-v2"
    
    # Ki·ªÉm tra d·ªØ li·ªáu ngu·ªìn
    if not os.path.exists(INPUT_DIR):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c d·ªØ li·ªáu: {INPUT_DIR}")
        print("üí° ƒê·∫£m b·∫£o c√≥ th∆∞ m·ª•c simple_dataset v·ªõi train/val/test")
        return
    
    print(f"üìÇ Input directory: {INPUT_DIR}")
    print(f"üìÇ Output base directory: {BASE_OUTPUT_DIR}")
    print()
    
    # X·ª≠ l√Ω PhoBERT-base
    print("üî• PH·∫¶N 1: PHOBERT-BASE")
    try:
        processor_base = VnCoreNLPPhoBERTPreprocessor(
            model_name="vinai/phobert-base",
            max_length=256
        )
        
        output_dir_base = os.path.join(BASE_OUTPUT_DIR, "phobert-base")
        processor_base.process_dataset(INPUT_DIR, output_dir_base)
        
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω PhoBERT-base: {e}")
        print("üí° Ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† dependencies")
    
    print("\n" + "="*70 + "\n")
    
    # X·ª≠ l√Ω PhoBERT-large
    print("üî• PH·∫¶N 2: PHOBERT-LARGE")
    try:
        processor_large = VnCoreNLPPhoBERTPreprocessor(
            model_name="vinai/phobert-large",
            max_length=256
        )
        
        output_dir_large = os.path.join(BASE_OUTPUT_DIR, "phobert-large")
        processor_large.process_dataset(INPUT_DIR, output_dir_large)
        
    except Exception as e:
        print(f"‚ùå L·ªói x·ª≠ l√Ω PhoBERT-large: {e}")
        print("üí° Ki·ªÉm tra l·∫°i c·∫•u h√¨nh v√† dependencies")
    
    print("\n" + "="*70)
    print("üéâ HO√ÄN TH√ÄNH T·∫§T C·∫¢!")
    print("üìÅ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† l∆∞u trong:")
    print(f"   - PhoBERT-base: {BASE_OUTPUT_DIR}/phobert-base/")
    print(f"   - PhoBERT-large: {BASE_OUTPUT_DIR}/phobert-large/")
    print("\nüí° B∆∞·ªõc ti·∫øp theo:")
    print("   python scripts/phobert/train_phobert.py --data_dir data-vncorenlp-v2")
    print("="*70)

if __name__ == "__main__":
    main() 